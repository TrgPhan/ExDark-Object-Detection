{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3560720,"sourceType":"datasetVersion","datasetId":2140079}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom PIL import Image\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import transforms, models\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\nfrom tqdm import tqdm\nos.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nimport torchvision.models.detection\nfrom torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights\nimport numpy as np\n\n\n# Đường dẫn dataset\nDATASET_DIR = '/kaggle/input/object-detection-exdark'\nIMAGES_DIR = os.path.join(DATASET_DIR, 'ExDark')\nANNOTATIONS_DIR = os.path.join(DATASET_DIR, 'ExDark_Annno')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Hàm đọc annotations từ cấu trúc thư mục phân cấp\ndef read_annotations(annotations_dir, images_dir):\n    annotations = {}\n    \n    # Duyệt qua tất cả các thư mục class trong ExDark_Annno\n    if not os.path.exists(annotations_dir):\n        print(f\"Annotations directory not found: {annotations_dir}\")\n        return {}\n    \n    for class_folder in os.listdir(annotations_dir):\n        class_anno_path = os.path.join(annotations_dir, class_folder)\n        if not os.path.isdir(class_anno_path):\n            continue\n            \n        # Duyệt qua thư mục con (có thể có thêm 1 lớp thư mục class)\n        for subfolder in os.listdir(class_anno_path):\n            subfolder_path = os.path.join(class_anno_path, subfolder)\n            if os.path.isdir(subfolder_path):\n                # Nếu có thêm 1 lớp thư mục con\n                annotation_files_path = subfolder_path\n                # print(f\"Found subfolder for class {class_folder}: {subfolder_path}\")\n            else:\n                # Nếu file annotation nằm trực tiếp trong thư mục class\n                annotation_files_path = class_anno_path\n                # print(f\"Using class folder for annotations: {class_anno_path}\")\n                break\n        \n        # Đọc các file annotation\n        if os.path.exists(annotation_files_path):\n            for filename in os.listdir(annotation_files_path):\n                if filename.endswith('.txt'):\n                    # Tìm file ảnh tương ứng\n                    img_name_base = filename.replace('.txt', '')\n                    \n                    # Tìm file ảnh trong thư mục class tương ứng\n                    img_class_path = os.path.join(images_dir, class_folder)\n                    img_path = None\n                    \n                    if os.path.exists(img_class_path):\n                        for img_file in os.listdir(img_class_path):\n                            if img_file.startswith(img_name_base):\n                                img_path = os.path.join(class_folder, img_file)\n                                # print(f\"Found image for annotation {filename}: {img_path}\")\n                                break\n                    \n                    if img_path is None:\n                        continue\n                    \n                    # Đọc annotations từ file txt\n                    anno_file_path = os.path.join(annotation_files_path, filename)\n                    with open(anno_file_path, 'r') as f:\n                        lines = f.readlines()\n                        objs = []\n                        \n                        for line in lines:\n                            line = line.strip()\n                            if line.startswith('%') or not line:  # Bỏ qua comment và dòng trống\n                                continue\n                                \n                            parts = line.split()\n                            if len(parts) < 7:  # Ít nhất cần có label, width, height, xmin, ymin, xmax, ymax\n                                continue\n                            \n                            try:\n                                label = parts[0]\n                                xmin = float(parts[1])\n                                ymin = float(parts[2])\n                                bbox_width = float(parts[3])\n                                bbox_height = float(parts[4])\n                                \n                                # Chuyển đổi sang format [x\n                                xmax = xmin + bbox_width\n                                ymax = ymin + bbox_height\n                                bbox = [xmin, ymin, xmax, ymax]\n                                \n                                objs.append({\n                                    'label': label,\n                                    'bbox': bbox\n                                })\n                                \n                                # print(f\"Parsed object: {label}, bbox: {bbox}, img_size: ({width}, {height})\")\n                            except ValueError:\n                                print(f\"Error parsing line in {filename}: {line}\")\n                                continue\n                        \n                        if objs:  # Chỉ thêm vào nếu có objects\n                            annotations[img_path] = objs\n    \n    print(f\"Loaded annotations for {len(annotations)} images\")\n    return annotations\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Hàm tạo label map từ annotations\ndef create_label_map(annotations):\n    label2idx = {}\n    idx = 1\n    for objs in annotations.values():\n        for obj in objs:\n            label = obj['label']\n            if label not in label2idx:\n                label2idx[label] = idx\n                idx += 1\n    return label2idx","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dataset cho object detection\nclass ExDarkDataset(Dataset):\n    def __init__(self, img_dir, annotations, label2idx, transform=None):\n        self.img_dir = img_dir\n        self.annotations = annotations\n        self.label2idx = label2idx\n        self.transform = transform\n        self.img_files = list(annotations.keys())\n        \n        print(f\"Dataset initialized with {len(self.img_files)} images\")\n        if len(self.img_files) > 0:\n            print(f\"Sample image paths: {self.img_files[:10]}\")\n\n    def __len__(self):\n        return len(self.img_files)\n\n    def __getitem__(self, idx):\n        img_file = self.img_files[idx]\n        img_full_path = os.path.join(self.img_dir, img_file)\n        \n        # Kiểm tra file có tồn tại không\n        if not os.path.exists(img_full_path):\n            print(f\"Image not found: {img_full_path}\")\n            # Thử tìm với các extension khác\n            base_path = os.path.splitext(img_full_path)[0]\n            for ext in ['.jpg', '.png', '.jpeg', '.JPG']:\n                if os.path.exists(base_path + ext):\n                    img_full_path = base_path + ext\n                    break\n        \n        try:\n            image = Image.open(img_full_path).convert(\"RGB\")\n        except Exception as e:\n            print(f\"Error loading image {img_full_path}: {e}\")\n            # Tạo ảnh trắng thay thế\n            image = Image.new('RGB', (224, 224), color='white')\n\n        objs = self.annotations[img_file]\n        boxes = []\n        labels = []\n        \n        for obj in objs:\n            bbox = obj['bbox']\n            # Đảm bảo bbox có định dạng [xmin, ymin, xmax, ymax] cho Faster R-CNN\n            xmin, ymin, xmax, ymax = bbox\n            boxes.append([xmin, ymin, xmax, ymax])\n            labels.append(self.label2idx[obj['label']])\n\n        # Chuyển đổi sang tensor\n        boxes = torch.tensor(boxes, dtype=torch.float32)\n        labels = torch.tensor(labels, dtype=torch.int64)\n        \n        # Tạo target dict\n        target = {}\n        target['boxes'] = boxes\n        target['labels'] = labels\n        target['image_id'] = torch.tensor([idx])\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, target\n\n# Transform ảnh\ntrain_transform = transforms.Compose([\n    transforms.ToTensor()\n])\n\n# Đọc annotations và tạo label map\nprint(\"Loading annotations...\")\nannotations = read_annotations(ANNOTATIONS_DIR, IMAGES_DIR)\n\nprint(\"Creating label map...\")\nlabel2idx = create_label_map(annotations)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Tạo dataset\nprint(\"Creating dataset...\")\ndataset = ExDarkDataset(\n    img_dir=IMAGES_DIR,\n    annotations=annotations,\n    label2idx=label2idx,\n    transform=train_transform\n)\nprint(f\"Dataset size: {len(dataset)}\")\ntry:\n    sample_img, sample_target = dataset[0]\n    print(f\"Sample image shape: {sample_img.shape}\")\n    print(f\"Sample target: {sample_target}\")\nexcept Exception as e:\n    print(f\"Error loading sample: {e}\")\n\n# Chia dataset train/val/test\ndef split_dataset(dataset, train_ratio=0.7, val_ratio=0.1):\n    total = len(dataset)\n    train_len = int(total * train_ratio)\n    val_len = int(total * val_ratio)\n    test_len = total - train_len - val_len\n    return random_split(dataset, [train_len, val_len, test_len])\n\ntrain_set, val_set, test_set = split_dataset(dataset)\nprint(f\"Train: {len(train_set)}, Val: {len(val_set)}, Test: {len(test_set)}\")\n\n# Custom collate function cho object detection\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\ntrain_loader = DataLoader(train_set, batch_size=4, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(val_set, batch_size=4, shuffle=False, collate_fn=collate_fn)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Sử dụng model Faster R-CNN cho object detection\n\ndef get_detection_model(num_classes):\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n    return model\n\nnum_classes = len(label2idx) + 1  # +1 cho background\nprint(f\"Number of classes: {num_classes}\")\n        \nmodel = get_detection_model(num_classes)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\nmodel.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calculate_iou(box1, box2):\n    \"\"\"Tính IoU giữa 2 bounding boxes\"\"\"\n    x1 = max(box1[0], box2[0])\n    y1 = max(box1[1], box2[1])\n    x2 = min(box1[2], box2[2])\n    y2 = min(box1[3], box2[3])\n    \n    if x2 <= x1 or y2 <= y1:\n        return 0.0\n    \n    intersection = (x2 - x1) * (y2 - y1)\n    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n    union = area1 + area2 - intersection\n    \n    return intersection / union\n\ndef calculate_ap(precisions, recalls):\n    \"\"\"Tính Average Precision\"\"\"\n    if len(precisions) == 0:\n        return 0.0\n    \n    recalls = np.concatenate(([0], recalls, [1]))\n    precisions = np.concatenate(([0], precisions, [0]))\n    \n    for i in range(len(precisions) - 2, -1, -1):\n        precisions[i] = max(precisions[i], precisions[i + 1])\n    \n    indices = np.where(recalls[1:] != recalls[:-1])[0] + 1\n    ap = np.sum((recalls[indices] - recalls[indices - 1]) * precisions[indices])\n    return ap\n\ndef evaluate_faster_rcnn(predictions, ground_truths):\n    \"\"\"Tính mAP cho Faster R-CNN\"\"\"\n    iou_threshold=0.5\n    # Tạo idx2label để map ngược lại tên class\n    idx2label = {v: k for k, v in label2idx.items()}\n    \n    # Lấy tất cả classes (loại bỏ background class = 0)\n    all_classes = set()\n    for gt in ground_truths:\n        all_classes.update(gt['labels'])\n    all_classes = sorted([c for c in all_classes if c > 0])  # Loại bỏ background\n    \n    if len(all_classes) == 0:\n        return 0.0\n    \n    class_aps = []\n    \n    for class_id in all_classes:\n        detections = []\n        num_gt = 0\n        \n        for img_idx, (pred, gt) in enumerate(zip(predictions, ground_truths)):\n            # Ground truth cho class này\n            gt_mask = gt['labels'] == class_id\n            gt_boxes_class = gt['boxes'][gt_mask]\n            num_gt += len(gt_boxes_class)\n            \n            # Predictions cho class này\n            pred_mask = pred['labels'] == class_id\n            pred_boxes_class = pred['boxes'][pred_mask]\n            pred_scores_class = pred['scores'][pred_mask]\n            \n            for box, score in zip(pred_boxes_class, pred_scores_class):\n                detections.append({\n                    'image_idx': img_idx,\n                    'box': box,\n                    'score': score,\n                    'gt_boxes': gt_boxes_class\n                })\n        \n        if num_gt == 0 or len(detections) == 0:\n            class_aps.append(0.0)\n            continue\n        \n        # Sắp xếp theo confidence\n        detections.sort(key=lambda x: x['score'], reverse=True)\n        \n        tp = np.zeros(len(detections))\n        fp = np.zeros(len(detections))\n        matched_gt = {}\n        \n        for det_idx, detection in enumerate(detections):\n            img_idx = detection['image_idx']\n            pred_box = detection['box']\n            gt_boxes = detection['gt_boxes']\n            \n            if img_idx not in matched_gt:\n                matched_gt[img_idx] = [False] * len(gt_boxes)\n            \n            max_iou = 0\n            max_gt_idx = -1\n            \n            for gt_idx, gt_box in enumerate(gt_boxes):\n                if not matched_gt[img_idx][gt_idx]:\n                    iou = calculate_iou(pred_box, gt_box)\n                    if iou > max_iou:\n                        max_iou = iou\n                        max_gt_idx = gt_idx\n            \n            if max_iou >= iou_threshold and max_gt_idx != -1:\n                tp[det_idx] = 1\n                matched_gt[img_idx][max_gt_idx] = True\n            else:\n                fp[det_idx] = 1\n        \n        tp_cumsum = np.cumsum(tp)\n        fp_cumsum = np.cumsum(fp)\n        \n        precisions = tp_cumsum / (tp_cumsum + fp_cumsum)\n        recalls = tp_cumsum / num_gt\n        \n        ap = calculate_ap(precisions, recalls)\n        class_aps.append(ap)\n        \n        # In ra tên class thay vì class ID\n        class_name = idx2label.get(class_id, f\"Class_{class_id}\")\n        print(f\"Class '{class_name}' (ID: {class_id}): AP = {ap:.4f}\")\n    \n    mAP = np.mean(class_aps) if class_aps else 0.0\n    return mAP","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate(model, test_loader):\n    \"\"\"Hàm đánh giá model - chỉ trả về mAP\"\"\"\n    model.eval()\n    predictions = []\n    ground_truths = []\n    \n    print(\"Starting evaluation...\")\n    \n    with torch.no_grad():\n        for batch_idx, (images, targets) in enumerate(tqdm(test_loader, desc=\"Evaluating\")):\n            # Chuyển images và targets lên device\n            images = [img.to(device) for img in images]\n            targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n            \n            # Forward pass\n            outputs = model(images)\n            \n            # Thu thập predictions và ground truths\n            for output, target in zip(outputs, targets):\n                # Lọc predictions có confidence > threshold\n                predictions.append({\n                    'boxes': output['boxes'].cpu().numpy(),\n                    'labels': output['labels'].cpu().numpy(),\n                    'scores': output['scores'].cpu().numpy()\n                })\n                \n                ground_truths.append({\n                    'boxes': target['boxes'].cpu().numpy(),\n                    'labels': target['labels'].cpu().numpy()\n                })\n    \n    print(f\"Collected {len(predictions)} predictions and {len(ground_truths)} ground truths\")\n    \n    # Tính mAP\n    mAP = evaluate_faster_rcnn(predictions, ground_truths)\n    \n    return mAP","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Lưu model\ndef save_model(model, path='/kaggle/working/object_detection_model.pth'):\n    torch.save(model.state_dict(), path)\n    print(f\"Model saved to {path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_losses = []\ntrain_accuracies = []\n\n# Hàm train model\ndef train_model(model, train_loader, val_loader, epochs=20, lr=7e-5, warmup_epochs=2, max_norm=1.0):\n    best_mAP = -float('inf')\n    # Initialize parameters and optimizer\n    params = [p for p in model.parameters() if p.requires_grad]\n    optimizer = optim.Adam(params, lr=lr, weight_decay=1e-4)\n    \n    # Learning rate scheduler\n    base_lr = lr\n    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n        optimizer, T_0=epochs, T_mult=1, eta_min=lr/100\n    )\n    for epoch in range(epochs):\n        model.train()\n        train_loss = 0\n        num_batches = 0\n        \n        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n        loop = tqdm(train_loader, desc=\"Training\")\n        \n        for images, targets in loop:\n            try:\n                # Move images and targets to device\n                images = [img.to(device) for img in images]\n                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n                \n                # Forward pass\n                loss_dict = model(images, targets)\n                losses = sum(loss for loss in loss_dict.values())\n                \n                # Backward pass\n                optimizer.zero_grad()\n                losses.backward()\n                \n                # Gradient clipping\n                torch.nn.utils.clip_grad_norm_(params, max_norm)\n                \n                optimizer.step()\n                \n                # Update loss\n                train_loss += losses.item()\n                num_batches += 1\n                \n                # Update progress bar\n                avg_loss = train_loss / num_batches if num_batches > 0 else 0\n                loop.set_postfix(loss=avg_loss)\n                \n            except Exception as e:\n                print(f\"Error in batch: {e}\")\n                continue\n        \n        # Step the scheduler\n        scheduler.step()\n        \n        # Calculate average loss\n        avg_loss = train_loss / num_batches if num_batches > 0 else 0\n        train_losses.append(avg_loss)\n        \n        # Evaluate on validation set if provided\n        val_accuracy = evaluate(model, val_loader)\n        if val_accuracy > best_mAP:\n            best_mAP = val_accuracy\n            save_model(model)\n            \n        train_accuracies.append(val_accuracy)\n        \n        print(f\"Epoch {epoch+1}/{epochs}, Avg Loss: {avg_loss:.4f}, mAP: {val_accuracy:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}\")\n    \n    return model, train_losses, train_accuracies","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model, train_losses, train_accuracies = train_model(model, train_loader, val_loader)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_loader = DataLoader(test_set, batch_size=4, shuffle=False, collate_fn=collate_fn)\nprint(evaluate(model, test_loader))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Hàm hiển thị kết quả prediction\ndef show_prediction(model, image, target=None, idx2label=None, confidence_threshold=0.5):\n    model.eval()\n    with torch.no_grad():\n        prediction = model([image.to(device)])[0]\n        \n    # Chuyển đổi bounding boxes về numpy để dễ vẽ\n    boxes = prediction['boxes'].cpu().numpy()\n    labels = prediction['labels'].cpu().numpy()\n    scores = prediction['scores'].cpu().numpy()\n    \n    # Vẽ bounding boxes\n    import matplotlib.pyplot as plt\n    import matplotlib.patches as patches\n    \n    fig, ax = plt.subplots(1, figsize=(12, 9))\n    \n    # Chuyển đổi image tensor về format để hiển thị\n    if image.shape[0] == 3:  # CHW format\n        img_display = image.permute(1, 2, 0).cpu().numpy()\n    else:  # HWC format\n        img_display = image.cpu().numpy()\n    \n    # Đảm bảo pixel values trong range [0, 1]\n    if img_display.max() > 1.0:\n        img_display = img_display / 255.0\n    \n    ax.imshow(img_display)\n    \n    # Vẽ prediction boxes (màu đỏ)\n    for box, label, score in zip(boxes, labels, scores):\n        if score < confidence_threshold:  # Chỉ vẽ nếu confidence > threshold\n            continue\n            \n        xmin, ymin, xmax, ymax = box\n        width = xmax - xmin\n        height = ymax - ymin\n        \n        rect = patches.Rectangle((xmin, ymin), width, height, linewidth=2,\n                                edgecolor='r', facecolor='none')\n        ax.add_patch(rect)\n        \n        # Hiển thị label và score\n        if idx2label and label in idx2label:\n            label_name = idx2label[label]\n        else:\n            label_name = f\"Class_{label}\"\n            \n        ax.text(xmin, ymin-5, f\"{label_name} ({score:.2f})\", \n                color='white', fontsize=10,\n                bbox=dict(facecolor='red', alpha=0.7))\n    \n    # Vẽ ground truth boxes nếu có (màu xanh lá)\n    if target is not None:\n        gt_boxes = target['boxes'].cpu().numpy()\n        gt_labels = target['labels'].cpu().numpy()\n        \n        for box, label in zip(gt_boxes, gt_labels):\n            xmin, ymin, xmax, ymax = box\n            width = xmax - xmin\n            height = ymax - ymin\n            \n            rect = patches.Rectangle((xmin, ymin), width, height, linewidth=2,\n                                    edgecolor='g', facecolor='none', linestyle='--')\n            ax.add_patch(rect)\n            \n            # Hiển thị ground truth label\n            if idx2label and label in idx2label:\n                label_name = idx2label[label]\n            else:\n                label_name = f\"Class_{label}\"\n                \n            ax.text(xmax, ymin-5, f\"GT: {label_name}\", \n                    color='white', fontsize=10,\n                    bbox=dict(facecolor='green', alpha=0.7))\n    \n    ax.set_title(\"Red: Predictions, Green: Ground Truth\")\n    plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n\n# Hiển thị prediction cho một ảnh trong test set\ndef show_test_prediction(model, test_set, idx=0, idx2label=None):\n    if idx < 0 or idx >= len(test_set):\n        print(f\"Index {idx} out of bounds for test set of size {len(test_set)}\")\n        return\n    \n    try:\n        image, target = test_set[idx]\n        show_prediction(model, image, target, idx2label)\n    except Exception as e:\n        print(f\"Error showing prediction for index {idx}: {e}\")\n\n# Hàm tiện ích để tạo idx2label từ label2idx\ndef create_idx2label(label2idx):\n    return {v: k for k, v in label2idx.items()}\n\n# Sử dụng:\n# Tạo mapping ngược từ index về tên class\nidx2label = create_idx2label(label2idx)\n\n# Hiển thị kết quả\nshow_test_prediction(model, test_set, idx=100, idx2label=idx2label)\nshow_test_prediction(model, test_set, idx=10, idx2label=idx2label)\nshow_test_prediction(model, test_set, idx=200, idx2label=idx2label)\nshow_test_prediction(model, test_set, idx=300, idx2label=idx2label)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Vẽ sơ đồ loss và accuracy\ndef plot_training_history(train_losses, train_accuracies):\n    epochs = range(1, len(train_losses) + 1)\n    \n    plt.figure(figsize=(12, 5))\n    \n    # Loss plot\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, train_losses, label='Training Loss', color='blue')\n    plt.title('Training Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.grid()\n    plt.legend()\n    \n    # Accuracy plot\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, train_accuracies, label='Validation mAP', color='orange')\n    plt.title('Validation mAP')\n    plt.xlabel('Epochs')\n    plt.ylabel('mAP')\n    plt.grid()\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}