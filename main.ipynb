{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-30T06:23:20.046433Z",
     "iopub.status.busy": "2025-05-30T06:23:20.045883Z",
     "iopub.status.idle": "2025-05-30T06:23:26.838742Z",
     "shell.execute_reply": "2025-05-30T06:23:26.838034Z",
     "shell.execute_reply.started": "2025-05-30T06:23:20.046414Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\n",
    "from tqdm import tqdm\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import torchvision.models.detection\n",
    "from torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Đường dẫn dataset\n",
    "DATASET_DIR = 'datasets'\n",
    "IMAGES_DIR = os.path.join(DATASET_DIR, 'ExDark')\n",
    "ANNOTATIONS_DIR = os.path.join(DATASET_DIR, 'ExDark_Annno')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T06:23:26.840219Z",
     "iopub.status.busy": "2025-05-30T06:23:26.839847Z",
     "iopub.status.idle": "2025-05-30T06:23:26.850890Z",
     "shell.execute_reply": "2025-05-30T06:23:26.850251Z",
     "shell.execute_reply.started": "2025-05-30T06:23:26.840193Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Hàm đọc annotations từ cấu trúc thư mục phân cấp\n",
    "def read_annotations(annotations_dir, images_dir):\n",
    "    annotations = {}\n",
    "    \n",
    "    # Duyệt qua tất cả các thư mục class trong ExDark_Annno\n",
    "    if not os.path.exists(annotations_dir):\n",
    "        print(f\"Annotations directory not found: {annotations_dir}\")\n",
    "        return {}\n",
    "    \n",
    "    for class_folder in os.listdir(annotations_dir):\n",
    "        class_anno_path = os.path.join(annotations_dir, class_folder)\n",
    "        if not os.path.isdir(class_anno_path):\n",
    "            continue\n",
    "            \n",
    "        # Duyệt qua thư mục con (có thể có thêm 1 lớp thư mục class)\n",
    "        for subfolder in os.listdir(class_anno_path):\n",
    "            subfolder_path = os.path.join(class_anno_path, subfolder)\n",
    "            if os.path.isdir(subfolder_path):\n",
    "                # Nếu có thêm 1 lớp thư mục con\n",
    "                annotation_files_path = subfolder_path\n",
    "                # print(f\"Found subfolder for class {class_folder}: {subfolder_path}\")\n",
    "            else:\n",
    "                # Nếu file annotation nằm trực tiếp trong thư mục class\n",
    "                annotation_files_path = class_anno_path\n",
    "                # print(f\"Using class folder for annotations: {class_anno_path}\")\n",
    "                break\n",
    "        \n",
    "        # Đọc các file annotation\n",
    "        if os.path.exists(annotation_files_path):\n",
    "            for filename in os.listdir(annotation_files_path):\n",
    "                if filename.endswith('.txt'):\n",
    "                    # Tìm file ảnh tương ứng\n",
    "                    img_name_base = filename.replace('.txt', '')\n",
    "                    \n",
    "                    # Tìm file ảnh trong thư mục class tương ứng\n",
    "                    img_class_path = os.path.join(images_dir, class_folder)\n",
    "                    img_path = None\n",
    "                    \n",
    "                    if os.path.exists(img_class_path):\n",
    "                        for img_file in os.listdir(img_class_path):\n",
    "                            if img_file.startswith(img_name_base):\n",
    "                                img_path = os.path.join(class_folder, img_file)\n",
    "                                # print(f\"Found image for annotation {filename}: {img_path}\")\n",
    "                                break\n",
    "                    \n",
    "                    if img_path is None:\n",
    "                        continue\n",
    "                    \n",
    "                    # Đọc annotations từ file txt\n",
    "                    anno_file_path = os.path.join(annotation_files_path, filename)\n",
    "                    with open(anno_file_path, 'r') as f:\n",
    "                        lines = f.readlines()\n",
    "                        objs = []\n",
    "                        \n",
    "                        for line in lines:\n",
    "                            line = line.strip()\n",
    "                            if line.startswith('%') or not line:  # Bỏ qua comment và dòng trống\n",
    "                                continue\n",
    "                                \n",
    "                            parts = line.split()\n",
    "                            if len(parts) < 7:  # Ít nhất cần có label, width, height, xmin, ymin, xmax, ymax\n",
    "                                continue\n",
    "                            \n",
    "                            try:\n",
    "                                label = parts[0]\n",
    "                                xmin = float(parts[1])\n",
    "                                ymin = float(parts[2])\n",
    "                                bbox_width = float(parts[3])\n",
    "                                bbox_height = float(parts[4])\n",
    "                                \n",
    "                                # Chuyển đổi sang format [x\n",
    "                                xmax = xmin + bbox_width\n",
    "                                ymax = ymin + bbox_height\n",
    "                                bbox = [xmin, ymin, xmax, ymax]\n",
    "                                \n",
    "                                objs.append({\n",
    "                                    'label': label,\n",
    "                                    'bbox': bbox\n",
    "                                })\n",
    "                                \n",
    "                                # print(f\"Parsed object: {label}, bbox: {bbox}, img_size: ({width}, {height})\")\n",
    "                            except ValueError:\n",
    "                                print(f\"Error parsing line in {filename}: {line}\")\n",
    "                                continue\n",
    "                        \n",
    "                        if objs:  # Chỉ thêm vào nếu có objects\n",
    "                            annotations[img_path] = objs\n",
    "    \n",
    "    print(f\"Loaded annotations for {len(annotations)} images\")\n",
    "    return annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T06:23:26.852925Z",
     "iopub.status.busy": "2025-05-30T06:23:26.852676Z",
     "iopub.status.idle": "2025-05-30T06:23:26.872727Z",
     "shell.execute_reply": "2025-05-30T06:23:26.872025Z",
     "shell.execute_reply.started": "2025-05-30T06:23:26.852905Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Hàm tạo label map từ annotations\n",
    "def create_label_map(annotations):\n",
    "    label2idx = {}\n",
    "    idx = 1  # Start from 1, not 0 (0 is reserved for background)\n",
    "    for objs in annotations.values():\n",
    "        for obj in objs:\n",
    "            label = obj['label']\n",
    "            if label not in label2idx:\n",
    "                label2idx[label] = idx\n",
    "                idx += 1\n",
    "    return label2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T06:23:26.873769Z",
     "iopub.status.busy": "2025-05-30T06:23:26.873531Z",
     "iopub.status.idle": "2025-05-30T06:24:09.703379Z",
     "shell.execute_reply": "2025-05-30T06:24:09.702621Z",
     "shell.execute_reply.started": "2025-05-30T06:23:26.873749Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading annotations...\n",
      "Loaded annotations for 7361 images\n",
      "Creating label map...\n"
     ]
    }
   ],
   "source": [
    "# Dataset cho object detection\n",
    "class ExDarkDataset(Dataset):\n",
    "    def __init__(self, img_dir, annotations, label2idx, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.annotations = annotations\n",
    "        self.label2idx = label2idx\n",
    "        self.transform = transform\n",
    "        self.img_files = list(annotations.keys())\n",
    "        \n",
    "        print(f\"Dataset initialized with {len(self.img_files)} images\")\n",
    "        if len(self.img_files) > 0:\n",
    "            print(f\"Sample image paths: {self.img_files[:10]}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_file = self.img_files[idx]\n",
    "        img_full_path = os.path.join(self.img_dir, img_file)\n",
    "        \n",
    "        # Kiểm tra file có tồn tại không\n",
    "        if not os.path.exists(img_full_path):\n",
    "            print(f\"Image not found: {img_full_path}\")\n",
    "            # Thử tìm với các extension khác\n",
    "            base_path = os.path.splitext(img_full_path)[0]\n",
    "            for ext in ['.jpg', '.png', '.jpeg', '.JPG']:\n",
    "                if os.path.exists(base_path + ext):\n",
    "                    img_full_path = base_path + ext\n",
    "                    break\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_full_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_full_path}: {e}\")\n",
    "            # Tạo ảnh trắng thay thế\n",
    "            image = Image.new('RGB', (224, 224), color='white')\n",
    "\n",
    "        objs = self.annotations[img_file]\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        for obj in objs:\n",
    "            bbox = obj['bbox']\n",
    "            # Đảm bảo bbox có định dạng [xmin, ymin, xmax, ymax] cho Faster R-CNN\n",
    "            xmin, ymin, xmax, ymax = bbox\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            labels.append(self.label2idx[obj['label']])\n",
    "\n",
    "        # Chuyển đổi sang tensor\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "        \n",
    "        # Tạo target dict\n",
    "        target = {}\n",
    "        target['boxes'] = boxes\n",
    "        target['labels'] = labels\n",
    "        target['image_id'] = torch.tensor([idx])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "# Transform ảnh\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Đọc annotations và tạo label map\n",
    "print(\"Loading annotations...\")\n",
    "annotations = read_annotations(ANNOTATIONS_DIR, IMAGES_DIR)\n",
    "\n",
    "print(\"Creating label map...\")\n",
    "label2idx = create_label_map(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T06:24:09.704399Z",
     "iopub.status.busy": "2025-05-30T06:24:09.704172Z",
     "iopub.status.idle": "2025-05-30T06:24:09.787334Z",
     "shell.execute_reply": "2025-05-30T06:24:09.786594Z",
     "shell.execute_reply.started": "2025-05-30T06:24:09.704383Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset...\n",
      "Dataset initialized with 7361 images\n",
      "Sample image paths: ['Bicycle\\\\2015_00001.png', 'Bicycle\\\\2015_00002.png', 'Bicycle\\\\2015_00003.png', 'Bicycle\\\\2015_00004.jpg', 'Bicycle\\\\2015_00005.jpg', 'Bicycle\\\\2015_00006.jpg', 'Bicycle\\\\2015_00007.jpg', 'Bicycle\\\\2015_00008.jpg', 'Bicycle\\\\2015_00009.jpg', 'Bicycle\\\\2015_00010.jpg']\n",
      "Dataset size: 7361\n",
      "Sample image shape: torch.Size([3, 375, 500])\n",
      "Sample target: {'boxes': tensor([[204.,  28., 475., 221.]]), 'labels': tensor([1]), 'image_id': tensor([0])}\n",
      "Train: 5152, Val: 736, Test: 1473\n"
     ]
    }
   ],
   "source": [
    "# Tạo dataset\n",
    "print(\"Creating dataset...\")\n",
    "dataset = ExDarkDataset(\n",
    "    img_dir=IMAGES_DIR,\n",
    "    annotations=annotations,\n",
    "    label2idx=label2idx,\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "try:\n",
    "    sample_img, sample_target = dataset[0]\n",
    "    print(f\"Sample image shape: {sample_img.shape}\")\n",
    "    print(f\"Sample target: {sample_target}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading sample: {e}\")\n",
    "\n",
    "# Chia dataset train/val/test\n",
    "def split_dataset(dataset, train_ratio=0.7, val_ratio=0.1):\n",
    "    total = len(dataset)\n",
    "    train_len = int(total * train_ratio)\n",
    "    val_len = int(total * val_ratio)\n",
    "    test_len = total - train_len - val_len\n",
    "    return random_split(dataset, [train_len, val_len, test_len])\n",
    "\n",
    "train_set, val_set, test_set = split_dataset(dataset)\n",
    "print(f\"Train: {len(train_set)}, Val: {len(val_set)}, Test: {len(test_set)}\")\n",
    "\n",
    "# Custom collate function cho object detection\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_set, batch_size=4, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T06:24:09.788934Z",
     "iopub.status.busy": "2025-05-30T06:24:09.788177Z",
     "iopub.status.idle": "2025-05-30T06:24:11.564841Z",
     "shell.execute_reply": "2025-05-30T06:24:11.564263Z",
     "shell.execute_reply.started": "2025-05-30T06:24:09.788909Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 13\n",
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=13, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=52, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sử dụng model Faster R-CNN cho object detection\n",
    "\n",
    "def get_detection_model(num_classes):\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "num_classes = len(label2idx) + 1  # +1 cho background\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "        \n",
    "model = get_detection_model(num_classes)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T06:24:11.565862Z",
     "iopub.status.busy": "2025-05-30T06:24:11.565550Z",
     "iopub.status.idle": "2025-05-30T06:24:11.577816Z",
     "shell.execute_reply": "2025-05-30T06:24:11.577329Z",
     "shell.execute_reply.started": "2025-05-30T06:24:11.565844Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"Tính IoU giữa 2 bounding boxes\"\"\"\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    \n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        return 0.0\n",
    "    \n",
    "    intersection = (x2 - x1) * (y2 - y1)\n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    union = area1 + area2 - intersection\n",
    "    \n",
    "    return intersection / union\n",
    "\n",
    "def calculate_ap(precisions, recalls):\n",
    "    \"\"\"Tính Average Precision\"\"\"\n",
    "    if len(precisions) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    recalls = np.concatenate(([0], recalls, [1]))\n",
    "    precisions = np.concatenate(([0], precisions, [0]))\n",
    "    \n",
    "    for i in range(len(precisions) - 2, -1, -1):\n",
    "        precisions[i] = max(precisions[i], precisions[i + 1])\n",
    "    \n",
    "    indices = np.where(recalls[1:] != recalls[:-1])[0] + 1\n",
    "    ap = np.sum((recalls[indices] - recalls[indices - 1]) * precisions[indices])\n",
    "    return ap\n",
    "\n",
    "def evaluate_faster_rcnn(predictions, ground_truths, iou_threshold=0.5):\n",
    "    \"\"\"Tính mAP cho Faster R-CNN\"\"\"\n",
    "    \n",
    "    # Tạo idx2label để map ngược lại tên class\n",
    "    idx2label = {v: k for k, v in label2idx.items()}\n",
    "    \n",
    "    # Lấy tất cả classes (loại bỏ background class = 0)\n",
    "    all_classes = set()\n",
    "    for gt in ground_truths:\n",
    "        all_classes.update(gt['labels'])\n",
    "    all_classes = sorted([c for c in all_classes if c > 0])  # Loại bỏ background\n",
    "    \n",
    "    if len(all_classes) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    class_aps = []\n",
    "    \n",
    "    for class_id in all_classes:\n",
    "        detections = []\n",
    "        num_gt = 0\n",
    "        \n",
    "        for img_idx, (pred, gt) in enumerate(zip(predictions, ground_truths)):\n",
    "            # Ground truth cho class này\n",
    "            gt_mask = gt['labels'] == class_id\n",
    "            gt_boxes_class = gt['boxes'][gt_mask]\n",
    "            num_gt += len(gt_boxes_class)\n",
    "            \n",
    "            # Predictions cho class này\n",
    "            pred_mask = pred['labels'] == class_id\n",
    "            pred_boxes_class = pred['boxes'][pred_mask]\n",
    "            pred_scores_class = pred['scores'][pred_mask]\n",
    "            \n",
    "            for box, score in zip(pred_boxes_class, pred_scores_class):\n",
    "                detections.append({\n",
    "                    'image_idx': img_idx,\n",
    "                    'box': box,\n",
    "                    'score': score,\n",
    "                    'gt_boxes': gt_boxes_class\n",
    "                })\n",
    "        \n",
    "        if num_gt == 0 or len(detections) == 0:\n",
    "            class_aps.append(0.0)\n",
    "            continue\n",
    "        \n",
    "        # Sắp xếp theo confidence\n",
    "        detections.sort(key=lambda x: x['score'], reverse=True)\n",
    "        \n",
    "        tp = np.zeros(len(detections))\n",
    "        fp = np.zeros(len(detections))\n",
    "        matched_gt = {}\n",
    "        \n",
    "        for det_idx, detection in enumerate(detections):\n",
    "            img_idx = detection['image_idx']\n",
    "            pred_box = detection['box']\n",
    "            gt_boxes = detection['gt_boxes']\n",
    "            \n",
    "            if img_idx not in matched_gt:\n",
    "                matched_gt[img_idx] = [False] * len(gt_boxes)\n",
    "            \n",
    "            max_iou = 0\n",
    "            max_gt_idx = -1\n",
    "            \n",
    "            for gt_idx, gt_box in enumerate(gt_boxes):\n",
    "                if not matched_gt[img_idx][gt_idx]:\n",
    "                    iou = calculate_iou(pred_box, gt_box)\n",
    "                    if iou > max_iou:\n",
    "                        max_iou = iou\n",
    "                        max_gt_idx = gt_idx\n",
    "            \n",
    "            if max_iou >= iou_threshold and max_gt_idx != -1:\n",
    "                tp[det_idx] = 1\n",
    "                matched_gt[img_idx][max_gt_idx] = True\n",
    "            else:\n",
    "                fp[det_idx] = 1\n",
    "        \n",
    "        tp_cumsum = np.cumsum(tp)\n",
    "        fp_cumsum = np.cumsum(fp)\n",
    "        \n",
    "        precisions = tp_cumsum / (tp_cumsum + fp_cumsum)\n",
    "        recalls = tp_cumsum / num_gt\n",
    "        \n",
    "        ap = calculate_ap(precisions, recalls)\n",
    "        class_aps.append(ap)\n",
    "        \n",
    "        # In ra tên class thay vì class ID\n",
    "        class_name = idx2label.get(class_id, f\"Class_{class_id}\")\n",
    "        print(f\"Class '{class_name}' (ID: {class_id}): AP = {ap:.4f}\")\n",
    "    \n",
    "    mAP = np.mean(class_aps) if class_aps else 0.0\n",
    "    return mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T06:24:11.578695Z",
     "iopub.status.busy": "2025-05-30T06:24:11.578538Z",
     "iopub.status.idle": "2025-05-30T06:24:11.604753Z",
     "shell.execute_reply": "2025-05-30T06:24:11.604089Z",
     "shell.execute_reply.started": "2025-05-30T06:24:11.578682Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    \"\"\"Hàm đánh giá model - chỉ trả về mAP\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    ground_truths = []\n",
    "    \n",
    "    print(\"Starting evaluation...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, targets) in enumerate(tqdm(test_loader, desc=\"Evaluating\")):\n",
    "            # Chuyển images và targets lên device\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Thu thập predictions và ground truths\n",
    "            for output, target in zip(outputs, targets):\n",
    "                # Lọc predictions có confidence > threshold\n",
    "                confidence_threshold = 0.5\n",
    "                keep_mask = output['scores'] > confidence_threshold\n",
    "                \n",
    "                predictions.append({\n",
    "                    'boxes': output['boxes'][keep_mask].cpu().numpy(),\n",
    "                    'labels': output['labels'][keep_mask].cpu().numpy(),\n",
    "                    'scores': output['scores'][keep_mask].cpu().numpy()\n",
    "                })\n",
    "                \n",
    "                ground_truths.append({\n",
    "                    'boxes': target['boxes'].cpu().numpy(),\n",
    "                    'labels': target['labels'].cpu().numpy()\n",
    "                })\n",
    "    \n",
    "    print(f\"Collected {len(predictions)} predictions and {len(ground_truths)} ground truths\")\n",
    "    \n",
    "    # Tính mAP\n",
    "    mAP = evaluate_faster_rcnn(predictions, ground_truths, iou_threshold=0.5)\n",
    "    print(f\"mAP@0.5: {mAP:.4f}\")\n",
    "    \n",
    "    return mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T06:24:11.606915Z",
     "iopub.status.busy": "2025-05-30T06:24:11.606696Z",
     "iopub.status.idle": "2025-05-30T06:24:11.625507Z",
     "shell.execute_reply": "2025-05-30T06:24:11.624854Z",
     "shell.execute_reply.started": "2025-05-30T06:24:11.606899Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "# Hàm train model\n",
    "def train_model(model, train_loader, val_loader, epochs=5, lr=1e-3, warmup_epochs=2, max_norm=1.0):\n",
    "    \n",
    "    # Initialize parameters and optimizer\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = optim.Adam(params, lr=lr, weight_decay=1e-4)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    base_lr = lr\n",
    "    warmup_scheduler = LinearLR(optimizer, start_factor=0.1, end_factor=1.0, total_iters=warmup_epochs)\n",
    "    main_scheduler = CosineAnnealingLR(optimizer, T_max=epochs - warmup_epochs, eta_min=base_lr / 100)\n",
    "    scheduler = SequentialLR(optimizer, schedulers=[warmup_scheduler, main_scheduler], milestones=[warmup_epochs])\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "        loop = tqdm(train_loader, desc=\"Training\")\n",
    "        \n",
    "        for images, targets in loop:\n",
    "            try:\n",
    "                # Move images and targets to device\n",
    "                images = [img.to(device) for img in images]\n",
    "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "                \n",
    "                # Forward pass\n",
    "                loss_dict = model(images, targets)\n",
    "                losses = sum(loss for loss in loss_dict.values())\n",
    "                \n",
    "                # Backward pass\n",
    "                optimizer.zero_grad()\n",
    "                losses.backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(params, max_norm)\n",
    "                \n",
    "                optimizer.step()\n",
    "                \n",
    "                # Update loss\n",
    "                train_loss += losses.item()\n",
    "                num_batches += 1\n",
    "                \n",
    "                # Update progress bar\n",
    "                avg_loss = train_loss / num_batches if num_batches > 0 else 0\n",
    "                loop.set_postfix(loss=avg_loss)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Step the scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Calculate average loss\n",
    "        avg_loss = train_loss / num_batches if num_batches > 0 else 0\n",
    "        train_losses.append(avg_loss)\n",
    "        \n",
    "        # Evaluate on validation set if provided\n",
    "        val_accuracy = evaluate(model, val_loader)\n",
    "        train_accuracies.append(val_accuracy)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Avg Loss: {avg_loss:.4f}, mAP: {val_accuracy:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "    \n",
    "    return model, train_losses, train_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T06:24:11.626284Z",
     "iopub.status.busy": "2025-05-30T06:24:11.626094Z",
     "iopub.status.idle": "2025-05-30T07:00:26.922587Z",
     "shell.execute_reply": "2025-05-30T07:00:26.921259Z",
     "shell.execute_reply.started": "2025-05-30T06:24:11.626270Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/1288 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|▌         | 74/1288 [18:52<5:09:35, 15.30s/it, loss=0.739]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model, train_losses, train_accuracies = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, epochs, lr, warmup_epochs, max_norm)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[32m     36\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[43mlosses\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Gradient clipping\u001b[39;00m\n\u001b[32m     40\u001b[39m torch.nn.utils.clip_grad_norm_(params, max_norm)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model, train_losses, train_accuracies = train_model(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-30T07:00:26.923201Z",
     "iopub.status.idle": "2025-05-30T07:00:26.923407Z",
     "shell.execute_reply": "2025-05-30T07:00:26.923319Z",
     "shell.execute_reply.started": "2025-05-30T07:00:26.923310Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_set, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "print(evaluate(model, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm hiển thị kết quả prediction\n",
    "def show_prediction(model, image, target=None, idx2label=None, confidence_threshold=0.5):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction = model([image.to(device)])[0]\n",
    "        \n",
    "    # Chuyển đổi bounding boxes về numpy để dễ vẽ\n",
    "    boxes = prediction['boxes'].cpu().numpy()\n",
    "    labels = prediction['labels'].cpu().numpy()\n",
    "    scores = prediction['scores'].cpu().numpy()\n",
    "    \n",
    "    # Vẽ bounding boxes\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.patches as patches\n",
    "    \n",
    "    fig, ax = plt.subplots(1, figsize=(12, 9))\n",
    "    \n",
    "    # Chuyển đổi image tensor về format để hiển thị\n",
    "    if image.shape[0] == 3:  # CHW format\n",
    "        img_display = image.permute(1, 2, 0).cpu().numpy()\n",
    "    else:  # HWC format\n",
    "        img_display = image.cpu().numpy()\n",
    "    \n",
    "    # Đảm bảo pixel values trong range [0, 1]\n",
    "    if img_display.max() > 1.0:\n",
    "        img_display = img_display / 255.0\n",
    "    \n",
    "    ax.imshow(img_display)\n",
    "    \n",
    "    # Vẽ prediction boxes (màu đỏ)\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        if score < confidence_threshold:  # Chỉ vẽ nếu confidence > threshold\n",
    "            continue\n",
    "            \n",
    "        xmin, ymin, xmax, ymax = box\n",
    "        width = xmax - xmin\n",
    "        height = ymax - ymin\n",
    "        \n",
    "        rect = patches.Rectangle((xmin, ymin), width, height, linewidth=2,\n",
    "                                edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Hiển thị label và score\n",
    "        if idx2label and label in idx2label:\n",
    "            label_name = idx2label[label]\n",
    "        else:\n",
    "            label_name = f\"Class_{label}\"\n",
    "            \n",
    "        ax.text(xmin, ymin-5, f\"{label_name} ({score:.2f})\", \n",
    "                color='white', fontsize=10,\n",
    "                bbox=dict(facecolor='red', alpha=0.7))\n",
    "    \n",
    "    # Vẽ ground truth boxes nếu có (màu xanh lá)\n",
    "    if target is not None:\n",
    "        gt_boxes = target['boxes'].cpu().numpy()\n",
    "        gt_labels = target['labels'].cpu().numpy()\n",
    "        \n",
    "        for box, label in zip(gt_boxes, gt_labels):\n",
    "            xmin, ymin, xmax, ymax = box\n",
    "            width = xmax - xmin\n",
    "            height = ymax - ymin\n",
    "            \n",
    "            rect = patches.Rectangle((xmin, ymin), width, height, linewidth=2,\n",
    "                                    edgecolor='g', facecolor='none', linestyle='--')\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            # Hiển thị ground truth label\n",
    "            if idx2label and label in idx2label:\n",
    "                label_name = idx2label[label]\n",
    "            else:\n",
    "                label_name = f\"Class_{label}\"\n",
    "                \n",
    "            ax.text(xmax, ymin-5, f\"GT: {label_name}\", \n",
    "                    color='white', fontsize=10,\n",
    "                    bbox=dict(facecolor='green', alpha=0.7))\n",
    "    \n",
    "    ax.set_title(\"Red: Predictions, Green: Ground Truth\")\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Hiển thị prediction cho một ảnh trong test set\n",
    "def show_test_prediction(model, test_set, idx=0, idx2label=None):\n",
    "    if idx < 0 or idx >= len(test_set):\n",
    "        print(f\"Index {idx} out of bounds for test set of size {len(test_set)}\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        image, target = test_set[idx]\n",
    "        show_prediction(model, image, target, idx2label)\n",
    "    except Exception as e:\n",
    "        print(f\"Error showing prediction for index {idx}: {e}\")\n",
    "\n",
    "# Hàm tiện ích để tạo idx2label từ label2idx\n",
    "def create_idx2label(label2idx):\n",
    "    return {v: k for k, v in label2idx.items()}\n",
    "\n",
    "# Sử dụng:\n",
    "# Tạo mapping ngược từ index về tên class\n",
    "idx2label = create_idx2label(label2idx)\n",
    "\n",
    "# Hiển thị kết quả\n",
    "show_test_prediction(model, test_set, idx=0, idx2label=idx2label)\n",
    "\n",
    "# Hoặc có thể hiển thị nhiều ảnh cùng lúc\n",
    "def show_multiple_predictions(model, test_set, num_images=4, idx2label=None):\n",
    "    \"\"\"Hiển thị prediction cho nhiều ảnh cùng lúc\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.patches as patches\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(min(num_images, len(test_set))):\n",
    "        try:\n",
    "            image, target = test_set[i]\n",
    "            \n",
    "            # Get prediction\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                prediction = model([image.to(device)])[0]\n",
    "            \n",
    "            boxes = prediction['boxes'].cpu().numpy()\n",
    "            labels = prediction['labels'].cpu().numpy()\n",
    "            scores = prediction['scores'].cpu().numpy()\n",
    "            \n",
    "            # Display image\n",
    "            ax = axes[i]\n",
    "            if image.shape[0] == 3:\n",
    "                img_display = image.permute(1, 2, 0).cpu().numpy()\n",
    "            else:\n",
    "                img_display = image.cpu().numpy()\n",
    "            \n",
    "            if img_display.max() > 1.0:\n",
    "                img_display = img_display / 255.0\n",
    "                \n",
    "            ax.imshow(img_display)\n",
    "            \n",
    "            # Draw predictions\n",
    "            for box, label, score in zip(boxes, labels, scores):\n",
    "                if score < 0.5:\n",
    "                    continue\n",
    "                    \n",
    "                xmin, ymin, xmax, ymax = box\n",
    "                width = xmax - xmin\n",
    "                height = ymax - ymin\n",
    "                \n",
    "                rect = patches.Rectangle((xmin, ymin), width, height, \n",
    "                                       linewidth=1, edgecolor='r', facecolor='none')\n",
    "                ax.add_patch(rect)\n",
    "                \n",
    "                if idx2label and label in idx2label:\n",
    "                    label_name = idx2label[label]\n",
    "                else:\n",
    "                    label_name = f\"C{label}\"\n",
    "                    \n",
    "                ax.text(xmin, ymin-2, f\"{label_name}\", \n",
    "                       color='white', fontsize=8,\n",
    "                       bbox=dict(facecolor='red', alpha=0.7))\n",
    "            \n",
    "            # Draw ground truth\n",
    "            if target:\n",
    "                gt_boxes = target['boxes'].cpu().numpy()\n",
    "                gt_labels = target['labels'].cpu().numpy()\n",
    "                \n",
    "                for box, label in zip(gt_boxes, gt_labels):\n",
    "                    xmin, ymin, xmax, ymax = box\n",
    "                    width = xmax - xmin\n",
    "                    height = ymax - ymin\n",
    "                    \n",
    "                    rect = patches.Rectangle((xmin, ymin), width, height, \n",
    "                                           linewidth=1, edgecolor='g', \n",
    "                                           facecolor='none', linestyle='--')\n",
    "                    ax.add_patch(rect)\n",
    "            \n",
    "            ax.set_title(f\"Test Image {i}\")\n",
    "            ax.axis('off')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with image {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Sử dụng:\n",
    "# show_multiple_predictions(model, test_set, num_images=4, idx2label=idx2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-30T07:00:26.924279Z",
     "iopub.status.idle": "2025-05-30T07:00:26.924534Z",
     "shell.execute_reply": "2025-05-30T07:00:26.924436Z",
     "shell.execute_reply.started": "2025-05-30T07:00:26.924423Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Lưu model\n",
    "def save_model(model, path='models/object_detection_model.pth'):\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "save_model(model)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 2140079,
     "sourceId": 3560720,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
