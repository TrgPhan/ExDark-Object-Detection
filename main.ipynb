{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3560720,"sourceType":"datasetVersion","datasetId":2140079}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom PIL import Image\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import transforms, models\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\nfrom tqdm import tqdm\nos.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nimport torchvision.models.detection\nfrom torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights\nimport numpy as np\n\n\n# Đường dẫn dataset\nDATASET_DIR = '/kaggle/input/object-detection-exdark'\nIMAGES_DIR = os.path.join(DATASET_DIR, 'ExDark')\nANNOTATIONS_DIR = os.path.join(DATASET_DIR, 'ExDark_Annno')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-30T06:23:20.045883Z","iopub.execute_input":"2025-05-30T06:23:20.046433Z","iopub.status.idle":"2025-05-30T06:23:26.838742Z","shell.execute_reply.started":"2025-05-30T06:23:20.046414Z","shell.execute_reply":"2025-05-30T06:23:26.838034Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Hàm đọc annotations từ cấu trúc thư mục phân cấp\ndef read_annotations(annotations_dir, images_dir):\n    annotations = {}\n    \n    # Duyệt qua tất cả các thư mục class trong ExDark_Annno\n    if not os.path.exists(annotations_dir):\n        print(f\"Annotations directory not found: {annotations_dir}\")\n        return {}\n    \n    for class_folder in os.listdir(annotations_dir):\n        class_anno_path = os.path.join(annotations_dir, class_folder)\n        if not os.path.isdir(class_anno_path):\n            continue\n            \n        # Duyệt qua thư mục con (có thể có thêm 1 lớp thư mục class)\n        for subfolder in os.listdir(class_anno_path):\n            subfolder_path = os.path.join(class_anno_path, subfolder)\n            if os.path.isdir(subfolder_path):\n                # Nếu có thêm 1 lớp thư mục con\n                annotation_files_path = subfolder_path\n                # print(f\"Found subfolder for class {class_folder}: {subfolder_path}\")\n            else:\n                # Nếu file annotation nằm trực tiếp trong thư mục class\n                annotation_files_path = class_anno_path\n                # print(f\"Using class folder for annotations: {class_anno_path}\")\n                break\n        \n        # Đọc các file annotation\n        if os.path.exists(annotation_files_path):\n            for filename in os.listdir(annotation_files_path):\n                if filename.endswith('.txt'):\n                    # Tìm file ảnh tương ứng\n                    img_name_base = filename.replace('.txt', '')\n                    \n                    # Tìm file ảnh trong thư mục class tương ứng\n                    img_class_path = os.path.join(images_dir, class_folder)\n                    img_path = None\n                    \n                    if os.path.exists(img_class_path):\n                        for img_file in os.listdir(img_class_path):\n                            if img_file.startswith(img_name_base):\n                                img_path = os.path.join(class_folder, img_file)\n                                # print(f\"Found image for annotation {filename}: {img_path}\")\n                                break\n                    \n                    if img_path is None:\n                        continue\n                    \n                    # Đọc annotations từ file txt\n                    anno_file_path = os.path.join(annotation_files_path, filename)\n                    with open(anno_file_path, 'r') as f:\n                        lines = f.readlines()\n                        objs = []\n                        \n                        for line in lines:\n                            line = line.strip()\n                            if line.startswith('%') or not line:  # Bỏ qua comment và dòng trống\n                                continue\n                                \n                            parts = line.split()\n                            if len(parts) < 7:  # Ít nhất cần có label, width, height, xmin, ymin, xmax, ymax\n                                continue\n                            \n                            try:\n                                label = parts[0]\n                                xmin = float(parts[1])\n                                ymin = float(parts[2])\n                                bbox_width = float(parts[3])\n                                bbox_height = float(parts[4])\n                                \n                                # Chuyển đổi sang format [x\n                                xmax = xmin + bbox_width\n                                ymax = ymin + bbox_height\n                                bbox = [xmin, ymin, xmax, ymax]\n                                \n                                objs.append({\n                                    'label': label,\n                                    'bbox': bbox\n                                })\n                                \n                                # print(f\"Parsed object: {label}, bbox: {bbox}, img_size: ({width}, {height})\")\n                            except ValueError:\n                                print(f\"Error parsing line in {filename}: {line}\")\n                                continue\n                        \n                        if objs:  # Chỉ thêm vào nếu có objects\n                            annotations[img_path] = objs\n    \n    print(f\"Loaded annotations for {len(annotations)} images\")\n    return annotations\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T06:23:26.839847Z","iopub.execute_input":"2025-05-30T06:23:26.840219Z","iopub.status.idle":"2025-05-30T06:23:26.850890Z","shell.execute_reply.started":"2025-05-30T06:23:26.840193Z","shell.execute_reply":"2025-05-30T06:23:26.850251Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Hàm tạo label map từ annotations\ndef create_label_map(annotations):\n    label2idx = {}\n    idx = 0\n    for objs in annotations.values():\n        for obj in objs:\n            label = obj['label']\n            if label not in label2idx:\n                label2idx[label] = idx\n                idx += 1\n    return label2idx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T06:23:26.852676Z","iopub.execute_input":"2025-05-30T06:23:26.852925Z","iopub.status.idle":"2025-05-30T06:23:26.872727Z","shell.execute_reply.started":"2025-05-30T06:23:26.852905Z","shell.execute_reply":"2025-05-30T06:23:26.872025Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Dataset cho object detection\nclass ExDarkDataset(Dataset):\n    def __init__(self, img_dir, annotations, label2idx, transform=None):\n        self.img_dir = img_dir\n        self.annotations = annotations\n        self.label2idx = label2idx\n        self.transform = transform\n        self.img_files = list(annotations.keys())\n        \n        print(f\"Dataset initialized with {len(self.img_files)} images\")\n        if len(self.img_files) > 0:\n            print(f\"Sample image paths: {self.img_files[:10]}\")\n\n    def __len__(self):\n        return len(self.img_files)\n\n    def __getitem__(self, idx):\n        img_file = self.img_files[idx]\n        img_full_path = os.path.join(self.img_dir, img_file)\n        \n        # Kiểm tra file có tồn tại không\n        if not os.path.exists(img_full_path):\n            print(f\"Image not found: {img_full_path}\")\n            # Thử tìm với các extension khác\n            base_path = os.path.splitext(img_full_path)[0]\n            for ext in ['.jpg', '.png', '.jpeg', '.JPG']:\n                if os.path.exists(base_path + ext):\n                    img_full_path = base_path + ext\n                    break\n        \n        try:\n            image = Image.open(img_full_path).convert(\"RGB\")\n        except Exception as e:\n            print(f\"Error loading image {img_full_path}: {e}\")\n            # Tạo ảnh trắng thay thế\n            image = Image.new('RGB', (224, 224), color='white')\n\n        objs = self.annotations[img_file]\n        boxes = []\n        labels = []\n        \n        for obj in objs:\n            bbox = obj['bbox']\n            # Đảm bảo bbox có định dạng [xmin, ymin, xmax, ymax] cho Faster R-CNN\n            xmin, ymin, xmax, ymax = bbox\n            boxes.append([xmin, ymin, xmax, ymax])\n            labels.append(self.label2idx[obj['label']])\n\n        # Chuyển đổi sang tensor\n        boxes = torch.tensor(boxes, dtype=torch.float32)\n        labels = torch.tensor(labels, dtype=torch.int64)\n        \n        # Tạo target dict\n        target = {}\n        target['boxes'] = boxes\n        target['labels'] = labels\n        target['image_id'] = torch.tensor([idx])\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, target\n\n# Transform ảnh\ntrain_transform = transforms.Compose([\n    transforms.ToTensor()\n])\n\n# Đọc annotations và tạo label map\nprint(\"Loading annotations...\")\nannotations = read_annotations(ANNOTATIONS_DIR, IMAGES_DIR)\n\nprint(\"Creating label map...\")\nlabel2idx = create_label_map(annotations)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T06:23:26.873531Z","iopub.execute_input":"2025-05-30T06:23:26.873769Z","iopub.status.idle":"2025-05-30T06:24:09.703379Z","shell.execute_reply.started":"2025-05-30T06:23:26.873749Z","shell.execute_reply":"2025-05-30T06:24:09.702621Z"}},"outputs":[{"name":"stdout","text":"Loading annotations...\nLoaded annotations for 7361 images\nCreating label map...\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Tạo dataset\nprint(\"Creating dataset...\")\ndataset = ExDarkDataset(\n    img_dir=IMAGES_DIR,\n    annotations=annotations,\n    label2idx=label2idx,\n    transform=train_transform\n)\nprint(f\"Dataset size: {len(dataset)}\")\ntry:\n    sample_img, sample_target = dataset[0]\n    print(f\"Sample image shape: {sample_img.shape}\")\n    print(f\"Sample target: {sample_target}\")\nexcept Exception as e:\n    print(f\"Error loading sample: {e}\")\n\n# Chia dataset train/val/test\ndef split_dataset(dataset, train_ratio=0.7, val_ratio=0.1):\n    total = len(dataset)\n    train_len = int(total * train_ratio)\n    val_len = int(total * val_ratio)\n    test_len = total - train_len - val_len\n    return random_split(dataset, [train_len, val_len, test_len])\n\ntrain_set, val_set, test_set = split_dataset(dataset)\nprint(f\"Train: {len(train_set)}, Val: {len(val_set)}, Test: {len(test_set)}\")\n\n# Custom collate function cho object detection\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\ntrain_loader = DataLoader(train_set, batch_size=2, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(val_set, batch_size=2, shuffle=False, collate_fn=collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T06:24:09.704172Z","iopub.execute_input":"2025-05-30T06:24:09.704399Z","iopub.status.idle":"2025-05-30T06:24:09.787334Z","shell.execute_reply.started":"2025-05-30T06:24:09.704383Z","shell.execute_reply":"2025-05-30T06:24:09.786594Z"}},"outputs":[{"name":"stdout","text":"Creating dataset...\nDataset initialized with 7361 images\nSample image paths: ['Motorbike/2015_06098.jpg', 'Motorbike/2015_06044.JPG', 'Motorbike/2015_05956.jpg', 'Motorbike/2015_05958.jpg', 'Motorbike/2015_06047.JPG', 'Motorbike/2015_05886.jpg', 'Motorbike/2015_06054.JPG', 'Motorbike/2015_06187.jpg', 'Motorbike/2015_05993.jpg', 'Motorbike/2015_06090.jpg']\nDataset size: 7361\nSample image shape: torch.Size([3, 746, 770])\nSample target: {'boxes': tensor([[304.,  84., 491., 347.],\n        [278., 258., 522., 721.]]), 'labels': tensor([0, 1]), 'image_id': tensor([0])}\nTrain: 5152, Val: 736, Test: 1473\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Sử dụng model Faster R-CNN cho object detection\n\ndef get_detection_model(num_classes):\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n    return model\n\nnum_classes = len(label2idx) + 1  # +1 cho background\nprint(f\"Number of classes: {num_classes}\")\n        \nmodel = get_detection_model(num_classes)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T06:24:09.788177Z","iopub.execute_input":"2025-05-30T06:24:09.788934Z","iopub.status.idle":"2025-05-30T06:24:11.564841Z","shell.execute_reply.started":"2025-05-30T06:24:09.788909Z","shell.execute_reply":"2025-05-30T06:24:11.564263Z"}},"outputs":[{"name":"stdout","text":"Number of classes: 13\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n100%|██████████| 160M/160M [00:00<00:00, 225MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"FasterRCNN(\n  (transform): GeneralizedRCNNTransform(\n      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n  )\n  (backbone): BackboneWithFPN(\n    (body): IntermediateLayerGetter(\n      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n      (relu): ReLU(inplace=True)\n      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n      (layer1): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): FrozenBatchNorm2d(256, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer2): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(512, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer3): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(1024, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (4): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (5): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer4): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(2048, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n    )\n    (fpn): FeaturePyramidNetwork(\n      (inner_blocks): ModuleList(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (2): Conv2dNormActivation(\n          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (3): Conv2dNormActivation(\n          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (layer_blocks): ModuleList(\n        (0-3): 4 x Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n      )\n      (extra_blocks): LastLevelMaxPool()\n    )\n  )\n  (rpn): RegionProposalNetwork(\n    (anchor_generator): AnchorGenerator()\n    (head): RPNHead(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): ReLU(inplace=True)\n        )\n      )\n      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n    )\n  )\n  (roi_heads): RoIHeads(\n    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n    (box_head): TwoMLPHead(\n      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n    )\n    (box_predictor): FastRCNNPredictor(\n      (cls_score): Linear(in_features=1024, out_features=13, bias=True)\n      (bbox_pred): Linear(in_features=1024, out_features=52, bias=True)\n    )\n  )\n)"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"def calculate_iou(box1, box2):\n    \"\"\"Tính IoU giữa 2 bounding boxes\"\"\"\n    x1 = max(box1[0], box2[0])\n    y1 = max(box1[1], box2[1])\n    x2 = min(box1[2], box2[2])\n    y2 = min(box1[3], box2[3])\n    \n    if x2 <= x1 or y2 <= y1:\n        return 0.0\n    \n    intersection = (x2 - x1) * (y2 - y1)\n    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n    union = area1 + area2 - intersection\n    \n    return intersection / union\n\ndef calculate_ap(precisions, recalls):\n    \"\"\"Tính Average Precision\"\"\"\n    if len(precisions) == 0:\n        return 0.0\n    \n    recalls = np.concatenate(([0], recalls, [1]))\n    precisions = np.concatenate(([0], precisions, [0]))\n    \n    for i in range(len(precisions) - 2, -1, -1):\n        precisions[i] = max(precisions[i], precisions[i + 1])\n    \n    indices = np.where(recalls[1:] != recalls[:-1])[0] + 1\n    ap = np.sum((recalls[indices] - recalls[indices - 1]) * precisions[indices])\n    return ap\n\ndef evaluate_faster_rcnn(predictions, ground_truths, iou_threshold=0.5):\n    \"\"\"Tính mAP cho Faster R-CNN\"\"\"\n    \n    # Tạo idx2label để map ngược lại tên class\n    idx2label = {v: k for k, v in label2idx.items()}\n    \n    # Lấy tất cả classes (loại bỏ background class = 0)\n    all_classes = set()\n    for gt in ground_truths:\n        all_classes.update(gt['labels'])\n    all_classes = sorted([c for c in all_classes if c > 0])  # Loại bỏ background\n    \n    if len(all_classes) == 0:\n        return 0.0\n    \n    class_aps = []\n    \n    for class_id in all_classes:\n        detections = []\n        num_gt = 0\n        \n        for img_idx, (pred, gt) in enumerate(zip(predictions, ground_truths)):\n            # Ground truth cho class này\n            gt_mask = gt['labels'] == class_id\n            gt_boxes_class = gt['boxes'][gt_mask]\n            num_gt += len(gt_boxes_class)\n            \n            # Predictions cho class này\n            pred_mask = pred['labels'] == class_id\n            pred_boxes_class = pred['boxes'][pred_mask]\n            pred_scores_class = pred['scores'][pred_mask]\n            \n            for box, score in zip(pred_boxes_class, pred_scores_class):\n                detections.append({\n                    'image_idx': img_idx,\n                    'box': box,\n                    'score': score,\n                    'gt_boxes': gt_boxes_class\n                })\n        \n        if num_gt == 0 or len(detections) == 0:\n            class_aps.append(0.0)\n            continue\n        \n        # Sắp xếp theo confidence\n        detections.sort(key=lambda x: x['score'], reverse=True)\n        \n        tp = np.zeros(len(detections))\n        fp = np.zeros(len(detections))\n        matched_gt = {}\n        \n        for det_idx, detection in enumerate(detections):\n            img_idx = detection['image_idx']\n            pred_box = detection['box']\n            gt_boxes = detection['gt_boxes']\n            \n            if img_idx not in matched_gt:\n                matched_gt[img_idx] = [False] * len(gt_boxes)\n            \n            max_iou = 0\n            max_gt_idx = -1\n            \n            for gt_idx, gt_box in enumerate(gt_boxes):\n                if not matched_gt[img_idx][gt_idx]:\n                    iou = calculate_iou(pred_box, gt_box)\n                    if iou > max_iou:\n                        max_iou = iou\n                        max_gt_idx = gt_idx\n            \n            if max_iou >= iou_threshold and max_gt_idx != -1:\n                tp[det_idx] = 1\n                matched_gt[img_idx][max_gt_idx] = True\n            else:\n                fp[det_idx] = 1\n        \n        tp_cumsum = np.cumsum(tp)\n        fp_cumsum = np.cumsum(fp)\n        \n        precisions = tp_cumsum / (tp_cumsum + fp_cumsum)\n        recalls = tp_cumsum / num_gt\n        \n        ap = calculate_ap(precisions, recalls)\n        class_aps.append(ap)\n        \n        # In ra tên class thay vì class ID\n        class_name = idx2label.get(class_id, f\"Class_{class_id}\")\n        print(f\"Class '{class_name}' (ID: {class_id}): AP = {ap:.4f}\")\n    \n    mAP = np.mean(class_aps) if class_aps else 0.0\n    return mAP","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T06:24:11.565550Z","iopub.execute_input":"2025-05-30T06:24:11.565862Z","iopub.status.idle":"2025-05-30T06:24:11.577816Z","shell.execute_reply.started":"2025-05-30T06:24:11.565844Z","shell.execute_reply":"2025-05-30T06:24:11.577329Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def evaluate(model, test_loader):\n    \"\"\"Hàm đánh giá model - chỉ trả về mAP\"\"\"\n    model.eval()\n    predictions = []\n    ground_truths = []\n    \n    print(\"Starting evaluation...\")\n    \n    with torch.no_grad():\n        for batch_idx, (images, targets) in enumerate(tqdm(test_loader, desc=\"Evaluating\")):\n            # Chuyển images và targets lên device\n            images = [img.to(device) for img in images]\n            targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n            \n            # Forward pass\n            outputs = model(images)\n            \n            # Thu thập predictions và ground truths\n            for output, target in zip(outputs, targets):\n                # Lọc predictions có confidence > threshold\n                confidence_threshold = 0.5\n                keep_mask = output['scores'] > confidence_threshold\n                \n                predictions.append({\n                    'boxes': output['boxes'][keep_mask].cpu().numpy(),\n                    'labels': output['labels'][keep_mask].cpu().numpy(),\n                    'scores': output['scores'][keep_mask].cpu().numpy()\n                })\n                \n                ground_truths.append({\n                    'boxes': target['boxes'].cpu().numpy(),\n                    'labels': target['labels'].cpu().numpy()\n                })\n    \n    print(f\"Collected {len(predictions)} predictions and {len(ground_truths)} ground truths\")\n    \n    # Tính mAP\n    mAP = evaluate_faster_rcnn(predictions, ground_truths, label2idx)\n    print(f\"mAP@0.5: {mAP:.4f}\")\n    \n    return mAP","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T06:24:11.578538Z","iopub.execute_input":"2025-05-30T06:24:11.578695Z","iopub.status.idle":"2025-05-30T06:24:11.604753Z","shell.execute_reply.started":"2025-05-30T06:24:11.578682Z","shell.execute_reply":"2025-05-30T06:24:11.604089Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"train_losses = []\ntrain_accuracies = []\n\n# Hàm train model\ndef train_model(model, train_loader, val_loader, epochs=5, lr=1e-3, warmup_epochs=5, max_norm=1.0):\n    \n    # Initialize parameters and optimizer\n    params = [p for p in model.parameters() if p.requires_grad]\n    optimizer = optim.Adam(params, lr=lr, weight_decay=1e-4)\n    \n    # Learning rate scheduler\n    base_lr = lr\n    warmup_scheduler = LinearLR(optimizer, start_factor=0.1, end_factor=1.0, total_iters=warmup_epochs)\n    main_scheduler = CosineAnnealingLR(optimizer, T_max=epochs - warmup_epochs, eta_min=base_lr / 100)\n    scheduler = SequentialLR(optimizer, schedulers=[warmup_scheduler, main_scheduler], milestones=[warmup_epochs])\n    \n    for epoch in range(epochs):\n        model.train()\n        train_loss = 0\n        num_batches = 0\n        \n        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n        loop = tqdm(train_loader, desc=\"Training\")\n        \n        for images, targets in loop:\n            try:\n                # Move images and targets to device\n                images = [img.to(device) for img in images]\n                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n                \n                # Forward pass\n                loss_dict = model(images, targets)\n                losses = sum(loss for loss in loss_dict.values())\n                \n                # Backward pass\n                optimizer.zero_grad()\n                losses.backward()\n                \n                # Gradient clipping\n                torch.nn.utils.clip_grad_norm_(params, max_norm)\n                \n                optimizer.step()\n                \n                # Update loss\n                train_loss += losses.item()\n                num_batches += 1\n                \n                # Update progress bar\n                avg_loss = train_loss / num_batches if num_batches > 0 else 0\n                loop.set_postfix(loss=avg_loss)\n                \n            except Exception as e:\n                print(f\"Error in batch: {e}\")\n                continue\n        \n        # Step the scheduler\n        scheduler.step()\n        \n        # Calculate average loss\n        avg_loss = train_loss / num_batches if num_batches > 0 else 0\n        train_losses.append(avg_loss)\n        \n        # Evaluate on validation set if provided\n        val_accuracy = evaluate(model, val_loader)\n        train_accuracies.append(val_accuracy)\n        \n        print(f\"Epoch {epoch+1}/{epochs}, Avg Loss: {avg_loss:.4f}, mAP: {val_accuracy:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}\")\n    \n    return model, train_losses, train_accuracies","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T06:24:11.606696Z","iopub.execute_input":"2025-05-30T06:24:11.606915Z","iopub.status.idle":"2025-05-30T06:24:11.625507Z","shell.execute_reply.started":"2025-05-30T06:24:11.606899Z","shell.execute_reply":"2025-05-30T06:24:11.624854Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"model, train_losses, train_accuracies = train_model(model, train_loader, val_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T06:24:11.626094Z","iopub.execute_input":"2025-05-30T06:24:11.626284Z","iopub.status.idle":"2025-05-30T07:00:26.922587Z","shell.execute_reply.started":"2025-05-30T06:24:11.626270Z","shell.execute_reply":"2025-05-30T07:00:26.921259Z"}},"outputs":[{"name":"stdout","text":"\nEpoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1288/1288 [17:04<00:00,  1.26it/s, loss=0.406]\n","output_type":"stream"},{"name":"stdout","text":"Starting evaluation...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 184/184 [01:15<00:00,  2.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Collected 736 predictions and 736 ground truths\nClass 1: AP = 0.2559\nClass 2: AP = 0.6639\nClass 3: AP = 0.3185\nClass 4: AP = 0.6654\nClass 5: AP = 0.3227\nClass 6: AP = 0.2342\nClass 7: AP = 0.1388\nClass 8: AP = 0.3484\nClass 9: AP = 0.1527\nClass 10: AP = 0.2728\nEpoch 1/20, Avg Loss: 0.4057, mAP: 0.2811, LR: 0.000280\n\nEpoch 2/20\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1288/1288 [16:31<00:00,  1.30it/s, loss=0.427]\n","output_type":"stream"},{"name":"stdout","text":"Starting evaluation...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 184/184 [01:12<00:00,  2.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Collected 736 predictions and 736 ground truths\nClass 1: AP = 0.1333\nClass 2: AP = 0.5104\nClass 3: AP = 0.4245\nClass 4: AP = 0.4415\nClass 5: AP = 0.1982\nClass 6: AP = 0.0992\nClass 7: AP = 0.0185\nClass 8: AP = 0.3302\nClass 9: AP = 0.2810\nClass 10: AP = 0.1166\nEpoch 2/20, Avg Loss: 0.4273, mAP: 0.2128, LR: 0.000460\n\nEpoch 3/20\n","output_type":"stream"},{"name":"stderr","text":"Training:   1%|          | 13/1288 [00:10<17:31,  1.21it/s, loss=0.441]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/1863234750.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_35/2132571061.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, epochs, lr, warmup_epochs, max_norm)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                 \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m                 \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroi_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_image_sizes\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[operator]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/rpn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, features, targets)\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0mobjectness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_bbox_deltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0manchors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manchor_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0mnum_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/anchor_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image_list, feature_maps)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrid_sizes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         ]\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_cell_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0manchors_over_all_feature_maps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0manchors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/anchor_utils.py\u001b[0m in \u001b[0;36mset_cell_anchors\u001b[0;34m(self, dtype, device)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_cell_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell_anchors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcell_anchor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcell_anchor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell_anchors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnum_anchors_per_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1990\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1991\u001b[0m                 \u001b[0mbuffers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_buffers\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1992\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBuffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbuffers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1993\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1994\u001b[0m                         raise TypeError(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parameter.py\u001b[0m in \u001b[0;36m__instancecheck__\u001b[0;34m(self, instance)\u001b[0m\n\u001b[1;32m    220\u001b[0m             ):\n\u001b[1;32m    221\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__instancecheck__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":10},{"cell_type":"code","source":"test_loader = DataLoader(test_set, batch_size=2, shuffle=False, collate_fn=collate_fn)\nprint(evaluate(model, test_loader))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:00:26.923201Z","iopub.status.idle":"2025-05-30T07:00:26.923407Z","shell.execute_reply.started":"2025-05-30T07:00:26.923310Z","shell.execute_reply":"2025-05-30T07:00:26.923319Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Lưu model\ndef save_model(model, path='models/object_detection_model.pth'):\n    torch.save(model.state_dict(), path)\n    print(f\"Model saved to {path}\")\n\nprint(\"Starting training...\")\nsave_model(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:00:26.924279Z","iopub.status.idle":"2025-05-30T07:00:26.924534Z","shell.execute_reply.started":"2025-05-30T07:00:26.924423Z","shell.execute_reply":"2025-05-30T07:00:26.924436Z"}},"outputs":[],"execution_count":null}]}